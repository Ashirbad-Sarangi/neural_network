{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36350b31-f335-440c-bb36-beb2387d208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as analytics\n",
    "import numpy as maths\n",
    "import math\n",
    "from numba import cuda , jit\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c68efa-5602-4bea-a046-92db220cd6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.930</td>\n",
       "      <td>-20.9020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.410</td>\n",
       "      <td>2.0033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-20.686</td>\n",
       "      <td>-33.3030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46.974</td>\n",
       "      <td>-13.5550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.965</td>\n",
       "      <td>20.3680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-29.232</td>\n",
       "      <td>28.4410</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>-10.159</td>\n",
       "      <td>7.3065</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-32.026</td>\n",
       "      <td>14.9380</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-49.533</td>\n",
       "      <td>-11.1280</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>-26.065</td>\n",
       "      <td>33.4200</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0       x1  y\n",
       "0    22.930 -20.9020  1\n",
       "1    12.410   2.0033  1\n",
       "2   -20.686 -33.3030  1\n",
       "3    46.974 -13.5550  1\n",
       "4    41.965  20.3680  1\n",
       "..      ...      ... ..\n",
       "195 -29.232  28.4410 -1\n",
       "196 -10.159   7.3065 -1\n",
       "197 -32.026  14.9380 -1\n",
       "198 -49.533 -11.1280 -1\n",
       "199 -26.065  33.4200 -1\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = analytics.read_csv('data1.csv',header = None , names = ['x0','x1','y'])\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "393dcb48-e0b2-4fe6-9ec8-3c16c51a8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network :\n",
    "\n",
    "    def __init__(self,df_data , number_of_hidden_layers = 1, number_of_neurons = [2,1], epsilon = 5e-2, alpha = 0.2) :\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_neurons = number_of_neurons\n",
    "        self.df_data = df_data\n",
    "        self.epsilon = epsilon \n",
    "        self.N = df_data.shape[0]\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "    def relu(self,x) :\n",
    "        \"\"\"Hidden Activation Function\"\"\"\n",
    "        return max(0,x)\n",
    "    \n",
    "    def sigmoid(self,w,x,b):\n",
    "        \"\"\"Output Function\"\"\"\n",
    "        return 1 / (1 + math.exp(-(w.T @ x + b)))\n",
    "\n",
    "    def initialisation(self):\n",
    "        weights = []\n",
    "        bias = []\n",
    "        for l in range(self.number_of_hidden_layers + 1):\n",
    "            if l == 0 : previous_layer = df_data.shape[1] - 1\n",
    "            else : previous_layer = number_of_neurons[l-1]\n",
    "            present_layer = number_of_neurons[l]\n",
    "            weights.append(maths.random.random((previous_layer,present_layer)))\n",
    "            bias.append(maths.random.random(present_layer).reshape(-1,1))\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "\n",
    "    def forward_propagation(self,x):\n",
    "        activations = []\n",
    "        activations.append(x)   # inputdata point is the initial activation\n",
    "        \n",
    "        hidden_layers = []\n",
    "        \n",
    "        for l in range(number_of_hidden_layers):\n",
    "            neurons = self.weights[l] @ activations[l] + self.bias[l]   # it actually should be activations[l+1] but because indexing in python starts from 0, so it is activations[l]\n",
    "            activation = maths.matrix([self.relu(float(neuron)) for neuron in neurons]).reshape(-1,1)\n",
    "            hidden_layers.append(neurons)\n",
    "            activations.append(activation)\n",
    "        activations.append(sigmoid(weights[-1],activations[-1], bias[-1]))\n",
    "        \n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activations = activations\n",
    "\n",
    "\n",
    "    def backward_propagation(self) :\n",
    "      \n",
    "        deltas = []\n",
    "        delta = maths.matrix(maths.multiply(self.activations[-1],  self.sigmoid(self.weights[-1],self.activations[-2], self.bias[-1]) * ( 1- self.sigmoid(self.weights[-1],self.activations[-2], self.bias[-1])))).reshape(-1,1)\n",
    "        deltas.append(delta)\n",
    "        \n",
    "        grad_weights = []\n",
    "        grad_biases = []\n",
    "        for l in range(self.number_of_hidden_layers , -1, -1 ): # it should be -1 only. Some errors in activations is not letting it run.\n",
    "            grad_weight = deltas[0] @ self.activations[l].T\n",
    "            grad_bias = deltas[0]\n",
    "            delta = maths.multiply((deltas[0] @ self.weights[1].T).T , maths.matrix([self.relu(float(i)) for i in self.hidden_layers[l-1]]).reshape(-1,1))\n",
    "            deltas.insert(0,delta)\n",
    "            grad_weights.append(grad_weight)\n",
    "            grad_biases.append(grad_bias)\n",
    "    \n",
    "        return grad_weights, grad_biases\n",
    "\n",
    "\n",
    "    def has_converged(self, prev_weights, prev_bias) :\n",
    "        self._converged = (maths.linalg.norm(maths.matrix(self.weights[0]) - maths.matrix(prev_weights[0])) < self.epsilon) and (maths.linalg.norm(maths.matrix(self.bias[0]) - maths.matrix(prev_bias[0])) < self.epsilon)\n",
    "\n",
    "\n",
    "    def update_weights(self):\n",
    "        \n",
    "        prev_weights = [w + 1 for w in self.weights]\n",
    "        prev_bias = [b + 1 for b in self.bias]\n",
    "        \n",
    "        self._converged = False\n",
    "        \n",
    "        while not self._converged:\n",
    "            prev_weights = weights.copy()\n",
    "            prev_bias = bias.copy()\n",
    "            \n",
    "            for m in range(self.N) :\n",
    "                x = maths.matrix(self.df_data.iloc[m][:-1]).reshape(-1,1)\n",
    "                y = self.df_data.iloc[m][-1]\n",
    "                self.forward_propagation(x)\n",
    "                grad_weights, grad_bias = self.backward_propagation()\n",
    "                for l in range(self.number_of_hidden_layers) :\n",
    "                    self.weights[l] = self.weights[l] - self.alpha/self.N * sum(grad_weights)\n",
    "                    self.bias[l] = self.bias[l] - self.alpha/self.N * sum(grad_bias)\n",
    "            \n",
    "            self.has_converged(prev_weights, prev_bias)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a30743-f1ac-4ea8-90b2-5774cc084b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = neural_network(df_data)\n",
    "ann.initialisation()\n",
    "ann.update_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d477c1-c070-4cb0-ad8b-1f0a091cfc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_neurons = [2,1]\n",
    "number_of_hidden_layers = 1\n",
    "\n",
    "def relu(x): \n",
    "    \"\"\"Hidden Activation Function\"\"\"\n",
    "    return max(0,x)\n",
    "\n",
    "def sigmoid(w,x,b):\n",
    "    \"\"\"Output Function\"\"\"\n",
    "    return 1 / (1 + math.exp(-(w.T @ x + b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d588a54-2218-4386-af01-47e907396daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[0.65513618, 0.48904618],\n",
       "         [0.25820238, 0.13249368]]),\n",
       "  array([[0.40336251],\n",
       "         [0.65869754]])],\n",
       " [array([[0.60863117],\n",
       "         [0.88536448]]),\n",
       "  array([[0.5163981]])])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = []\n",
    "bias = []\n",
    "for l in range(number_of_hidden_layers + 1):\n",
    "    if l == 0 : previous_layer = df_data.shape[1] - 1\n",
    "    else : previous_layer = number_of_neurons[l-1]\n",
    "    present_layer = number_of_neurons[l]\n",
    "    weights.append(maths.random.random((previous_layer,present_layer)))\n",
    "    bias.append(maths.random.random(present_layer).reshape(-1,1))\n",
    "    \n",
    "weights , bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3fddff0-dd6e-461f-9878-cda5386fe907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[ 22.93 ],\n",
       "         [-20.902]]),\n",
       " 1.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = maths.matrix(df_data.iloc[0][:-1]).reshape(2,1)\n",
    "y = df_data.iloc[0][-1]\n",
    "x , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "020116dd-0196-4f27-a587-286a1516a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit(target_backend = 'cuda')\n",
    "def forward_propagation(x):\n",
    "    activations = []\n",
    "    activations.append(x)   # activations has the first input as the input data point\n",
    "    \n",
    "    hidden_layers = []\n",
    "    \n",
    "    for l in range(number_of_hidden_layers):\n",
    "        neurons = weights[l] @ activations[l] + bias[l]   # it actually should be activations[l+1] but because indexing in python starts from 0, so it is activations[l]\n",
    "        activation = maths.matrix([relu(float(neuron)) for neuron in neurons]).reshape(-1,1)\n",
    "        hidden_layers.append(neurons)\n",
    "        activations.append(activation)\n",
    "    activations.append(sigmoid(weights[-1],activations[-1], bias[-1]))\n",
    "    return hidden_layers, activations\n",
    "\n",
    "# @jit(target_backend = 'cuda')\n",
    "def backward_propagation(hidden_layers, activations) :\n",
    "    deltas = []\n",
    "    delta = maths.matrix(maths.multiply(activations[-1],  sigmoid(weights[-1],activations[-2], bias[-1]) * ( 1- sigmoid(weights[-1],activations[-2], bias[-1])))).reshape(-1,1)\n",
    "    deltas.append(delta)\n",
    "    \n",
    "    grad_weights = []\n",
    "    grad_biases = []\n",
    "    for l in range(number_of_hidden_layers , -1, -1 ): # it should be -1 only. Some errors in activations is not letting it run.\n",
    "        grad_weight = deltas[0] @ activations[l].T\n",
    "        grad_bias = deltas[0]\n",
    "        delta = maths.multiply((deltas[0] @ weights[1].T).T , maths.matrix([relu(float(i)) for i in hidden_layers[l-1]]).reshape(-1,1))\n",
    "        deltas.insert(0,delta)\n",
    "        grad_weights.append(grad_weight)\n",
    "        grad_biases.append(grad_bias)\n",
    "\n",
    "    return grad_weights, grad_biases\n",
    "    \n",
    "# @jit(target_backend = 'cuda')\n",
    "def has_converged(prev_weights, prev_bias, weights, bias) :\n",
    "    converged = False\n",
    "    epsilon = 2e-2\n",
    "    converged = (maths.linalg.norm(maths.matrix(weights[0]) - maths.matrix(prev_weights[0])) < epsilon) and (maths.linalg.norm(maths.matrix(bias[0]) - maths.matrix(prev_bias[0])) < epsilon)\n",
    "    return converged\n",
    "\n",
    "# @jit(target_backend = 'cuda')\n",
    "def update_weights(df_data):\n",
    "    alpha = 0.2\n",
    "    N = df_data.shape[0]\n",
    "    \n",
    "    prev_weights = [w + 1 for w in weights]\n",
    "    prev_bias = [b + 1 for b in bias]\n",
    "    \n",
    "    converged = False\n",
    "    \n",
    "    \n",
    "    while not converged:\n",
    "        prev_weights = weights.copy()\n",
    "        prev_bias = bias.copy()\n",
    "        \n",
    "        for m in range(df_data.shape[0]) :\n",
    "            x = maths.matrix(df_data.iloc[m][:-1]).reshape(-1,1)\n",
    "            y = df_data.iloc[m][-1]\n",
    "            hidden_layers , activations = forward_propagation(x)\n",
    "            grad_weights, grad_bias = backward_propagation(hidden_layers, activations)\n",
    "            for l in range(number_of_hidden_layers) :\n",
    "                weights[l] = weights[l] - alpha/N * sum(grad_weights)\n",
    "                bias[l] = bias[l] - alpha/N * sum(grad_bias)\n",
    "        \n",
    "        converged = has_converged(prev_weights, prev_bias, weights, bias)\n",
    "    \n",
    "    print(weights)\n",
    "    print()\n",
    "    print(bias)\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddd70476-b070-43b9-bf12-57aa86e9a9e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55932/4163693319.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @jit(target_backend = 'cuda')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_55932/380081671.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(df_data)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprev_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mhidden_layers\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mgrad_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_deprecated_callable_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_callable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3986\u001b[0m             \u001b[0;31m# if we are a copy, mark as such\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3987\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnew_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3988\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3989\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3990\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3991\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3992\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, other, method, **kwargs)\u001b[0m\n\u001b[1;32m   6248\u001b[0m                \u001b[0mThe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcurrently\u001b[0m \u001b[0mconsidered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6249\u001b[0m                \u001b[0mstable\u001b[0m \u001b[0macross\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mreleases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6250\u001b[0m         \"\"\"\n\u001b[1;32m   6251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m                 \u001b[0;31m# We want attrs propagation to have minimal performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m                 \u001b[0;31m# impact if attrs are not used; i.e. attrs is an empty dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6255\u001b[0m                 \u001b[0;31m# One could make the deepcopy unconditionally, but a deepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \"\"\"\n\u001b[1;32m    365\u001b[0m         \u001b[0mDictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mattributes\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# @jit(target_backend = 'cuda')\n",
    "update_weights(df_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
